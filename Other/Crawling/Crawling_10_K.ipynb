{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 配置参数\n",
    "INDEX_CSV_PATH = r'C:\\Users\\lishe\\Documents\\GitHub\\Programming-Study\\Other\\putnc736bnq5aeat.csv'  # 替换为您的索引文件路径\n",
    "SAVE_DIR = 'sec_filings'      # 下载文件的保存目录\n",
    "USER_AGENT = 'Your Name (your.email@example.com)'  # 替换为您的姓名和邮箱，确保仅包含ASCII字符\n",
    "\n",
    "# 创建保存目录（如果不存在）\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# 读取索引数据\n",
    "try:\n",
    "    df = pd.read_csv(INDEX_CSV_PATH, dtype=str)\n",
    "    df.columns = df.columns.str.strip().str.lower()  # 清理和标准化列名\n",
    "    print(f\"成功读取索引文件：{INDEX_CSV_PATH}\")\n",
    "    print(f\"列名：{df.columns.tolist()}\")  # 检查列名\n",
    "except Exception as e:\n",
    "    print(f\"读取索引文件失败：{e}\")\n",
    "    exit(1)\n",
    "\n",
    "# 确保表格包含必要的列\n",
    "if 'cik' not in df.columns or 'form' not in df.columns:\n",
    "    print(\"索引文件缺少必要的列：'cik' 或 'form'\")\n",
    "    exit(1)\n",
    "\n",
    "# 下载文件的通用函数，并验证内容\n",
    "def download_and_validate_file(file_url, cik, filing_date, form_type, extension='html'):\n",
    "    headers = {'User-Agent': USER_AGENT}\n",
    "    response = requests.get(file_url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        file_name = f'{cik}_{filing_date}_{form_type}.{extension}'\n",
    "        file_path = os.path.join(SAVE_DIR, file_name)\n",
    "        with open(file_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f'文件已保存至：{file_path}')\n",
    "\n",
    "        # 检查文件内容是否包含 \"FORM 10-K\"、\"10-K\" 或 \"ANNUAL REPORT\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                for i in range(500):  # 读取前500行，以增加检索的范围\n",
    "                    line = file.readline().upper()\n",
    "                    if 'FORM 10-K' in line or '10-K' in line or 'ANNUAL REPORT' in line:\n",
    "                        print(f'文件包含FORM 10-K：{file_path}')\n",
    "                        return True  # 保留文件\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"无法解码文件：{file_path}，跳过进一步检查。\")\n",
    "        \n",
    "        # 如果文件前500行不包含关键字，则删除文件\n",
    "        print(f'文件不包含FORM 10-K或相关内容，已删除：{file_path}')\n",
    "        os.remove(file_path)\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"无法下载文件：{file_url}，状态码：{response.status_code}\")\n",
    "        return False\n",
    "\n",
    "# 遍历 CSV 中的每个 CIK 和 Form\n",
    "for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"处理每个公司\"):\n",
    "    cik = row['cik'].strip()\n",
    "    form_type = row['form'].strip().upper()\n",
    "\n",
    "    if form_type != '10-K':\n",
    "        continue  # 只处理10-K文件\n",
    "\n",
    "    # 构建公司档案页面URL\n",
    "    company_url = f'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK={cik}&type={form_type}&count=100&owner=exclude&output=xml'\n",
    "    headers = {'User-Agent': USER_AGENT}\n",
    "    response = requests.get(company_url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(f'无法访问公司页面：{company_url}，状态码：{response.status_code}')\n",
    "        continue\n",
    "\n",
    "    # 解析XML响应\n",
    "    soup = BeautifulSoup(response.content, 'lxml')\n",
    "    filings = soup.find_all('filing')\n",
    "\n",
    "    # 遍历每个提交\n",
    "    for filing in filings:\n",
    "        filing_date = filing.find('datefiled').text\n",
    "        filing_href = filing.find('filinghref').text\n",
    "        print(f'发现提交日期为{filing_date}的{form_type}文件：{filing_href}')\n",
    "\n",
    "        # 访问提交详情页面\n",
    "        response = requests.get(filing_href, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            print(f'无法访问提交详情页面，状态码：{response.status_code}')\n",
    "            continue\n",
    "\n",
    "        # 解析详情页面，查找所有文件表格\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        doc_tables = soup.find_all('table', class_='tableFile')\n",
    "\n",
    "        # 若存在多个tableFile表格，逐个解析\n",
    "        found_10k = False\n",
    "        for doc_table in doc_tables:\n",
    "            rows = doc_table.find_all('tr')\n",
    "            print(f'解析文件表格，找到 {len(rows)} 行记录')\n",
    "            for row in rows:\n",
    "                columns = row.find_all('td')\n",
    "                if len(columns) >= 4:\n",
    "                    doc_type = columns[3].text.strip().upper()\n",
    "                    doc_link = columns[2].find('a', href=True)\n",
    "                    \n",
    "                    if doc_type == '10-K' and doc_link:\n",
    "                        found_10k = True\n",
    "                        doc_href = doc_link['href']\n",
    "                        \n",
    "                        # 如果链接包含 \"/ix?doc=\"，则移除该部分\n",
    "                        if '/ix?doc=' in doc_href:\n",
    "                            doc_href = doc_href.replace('/ix?doc=', '')\n",
    "\n",
    "                        file_url = f'https://www.sec.gov{doc_href}'\n",
    "                        print(f'正在下载10-K文件：{file_url}')\n",
    "\n",
    "                        # 根据文件扩展名下载并验证内容\n",
    "                        if doc_href.endswith('.htm') or doc_href.endswith('.html'):\n",
    "                            download_and_validate_file(file_url, cik, filing_date, form_type, extension='html')\n",
    "                        elif doc_href.endswith('.txt'):\n",
    "                            download_and_validate_file(file_url, cik, filing_date, form_type, extension='txt')\n",
    "                        else:\n",
    "                            print(f'未找到合适的文件链接：{doc_href}')\n",
    "            if found_10k:\n",
    "                break  # 如果找到10-K，跳出内层循环\n",
    "        if not found_10k:\n",
    "            print(f'未在提交日期为{filing_date}的文件中找到10-K文件类型')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fixed date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This version is useable but need to fix print too much issue\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import fitz  # PyMuPDF for PDF text extraction\n",
    "\n",
    "# Configuration\n",
    "INDEX_CSV_PATH = r'C:\\Users\\lishe\\Documents\\GitHub\\Programming-Study\\Other\\putnc736bnq5aeat.csv'  # Replace with your index file path\n",
    "SAVE_DIR = 'sec_filings'      # Directory to save downloaded files\n",
    "USER_AGENT = 'Your Name (your.email@example.com)'  # Replace with your name and email (ASCII only)\n",
    "\n",
    "# Create save directory if it doesn't exist\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Read the index data\n",
    "try:\n",
    "    df = pd.read_csv(INDEX_CSV_PATH, dtype=str)\n",
    "    df.columns = df.columns.str.strip().str.lower()  # Clean and standardize column names\n",
    "    print(f\"Successfully read index file: {INDEX_CSV_PATH}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")  # Check column names\n",
    "except Exception as e:\n",
    "    print(f\"Failed to read index file: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Ensure required columns exist\n",
    "if 'cik' not in df.columns or 'form' not in df.columns or 'fdate' not in df.columns:\n",
    "    print(\"Index file missing required columns: 'cik', 'form', or 'fdate'\")\n",
    "    exit(1)\n",
    "\n",
    "# Download and validate function\n",
    "def download_and_validate_file(file_url, cik, filing_date, form_type, extension='html'):\n",
    "    headers = {'User-Agent': USER_AGENT}\n",
    "    response = requests.get(file_url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        file_name = f'{cik}_{filing_date}_{form_type}.{extension}'\n",
    "        file_path = os.path.join(SAVE_DIR, file_name)\n",
    "        with open(file_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f'File saved to: {file_path}')\n",
    "\n",
    "        # Check content for \"FORM 10-K\", \"10-K\", or \"ANNUAL REPORT\"\n",
    "        if extension == 'pdf':\n",
    "            try:\n",
    "                with fitz.open(file_path) as pdf:\n",
    "                    for page_num in range(min(5, pdf.page_count)):  # Check the first 5 pages\n",
    "                        text = pdf[page_num].get_text()\n",
    "                        if any(keyword in text.upper() for keyword in ['FORM 10-K', '10-K', 'ANNUAL REPORT']):\n",
    "                            print(f'File contains FORM 10-K: {file_path}')\n",
    "                            return True\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading PDF file: {file_path}, Error: {e}\")\n",
    "        else:\n",
    "            # Check HTML or TXT content\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                    for _ in range(500):  # Read first 500 lines for keywords\n",
    "                        line = file.readline().upper()\n",
    "                        if 'FORM 10-K' in line or '10-K' in line or 'ANNUAL REPORT' in line:\n",
    "                            print(f'File contains FORM 10-K: {file_path}')\n",
    "                            return True\n",
    "            except UnicodeDecodeError:\n",
    "                print(f\"Cannot decode file: {file_path}, skipping further check.\")\n",
    "        \n",
    "        # Delete file if it doesn't contain the necessary content\n",
    "        print(f'File does not contain FORM 10-K or related content, deleting: {file_path}')\n",
    "        os.remove(file_path)\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"Failed to download file: {file_url}, Status Code: {response.status_code}\")\n",
    "        return False\n",
    "\n",
    "# Process each CIK and Form in the CSV\n",
    "for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing each company\"):\n",
    "    cik = row['cik'].strip()\n",
    "    form_type = row['form'].strip().upper()\n",
    "    fdate = row['fdate'].strip()  # Expected filing date\n",
    "\n",
    "    if form_type != '10-K':\n",
    "        continue  # Only process 10-K files\n",
    "\n",
    "    # Build company filing page URL\n",
    "    company_url = f'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK={cik}&type={form_type}&count=100&owner=exclude&output=xml'\n",
    "    headers = {'User-Agent': USER_AGENT}\n",
    "    response = requests.get(company_url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(f'Cannot access company page: {company_url}, Status Code: {response.status_code}')\n",
    "        continue\n",
    "\n",
    "    # Parse XML response\n",
    "    soup = BeautifulSoup(response.content, 'lxml')\n",
    "    filings = soup.find_all('filing')\n",
    "\n",
    "    # Iterate through each filing\n",
    "    for filing in filings:\n",
    "        filing_date = filing.find('datefiled').text\n",
    "        if filing_date != fdate:\n",
    "            continue  # Skip files that don't match the fdate\n",
    "\n",
    "        filing_href = filing.find('filinghref').text\n",
    "        print(f'Found {form_type} file dated {filing_date}: {filing_href}')\n",
    "\n",
    "        # Access filing detail page\n",
    "        response = requests.get(filing_href, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            print(f'Cannot access filing detail page, Status Code: {response.status_code}')\n",
    "            continue\n",
    "\n",
    "        # Parse filing detail page and search for file table\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        doc_tables = soup.find_all('table', class_='tableFile')\n",
    "\n",
    "        # Process each document table for 10-K files\n",
    "        found_10k = False\n",
    "        for doc_table in doc_tables:\n",
    "            rows = doc_table.find_all('tr')\n",
    "            print(f'Parsing document table with {len(rows)} rows')\n",
    "            for row in rows:\n",
    "                columns = row.find_all('td')\n",
    "                if len(columns) >= 4:\n",
    "                    doc_type = columns[3].text.strip().upper()\n",
    "                    doc_link = columns[2].find('a', href=True)\n",
    "                    \n",
    "                    if doc_type == '10-K' and doc_link:\n",
    "                        found_10k = True\n",
    "                        doc_href = doc_link['href']\n",
    "                        \n",
    "                        # Remove \"/ix?doc=\" if present\n",
    "                        if '/ix?doc=' in doc_href:\n",
    "                            doc_href = doc_href.replace('/ix?doc=', '')\n",
    "\n",
    "                        file_url = f'https://www.sec.gov{doc_href}'\n",
    "                        print(f'Downloading 10-K file: {file_url}')\n",
    "\n",
    "                        # Determine file extension and download\n",
    "                        if doc_href.endswith('.htm') or doc_href.endswith('.html'):\n",
    "                            download_and_validate_file(file_url, cik, filing_date, form_type, extension='html')\n",
    "                        elif doc_href.endswith('.txt'):\n",
    "                            download_and_validate_file(file_url, cik, filing_date, form_type, extension='txt')\n",
    "                        elif doc_href.endswith('.pdf'):\n",
    "                            download_and_validate_file(file_url, cik, filing_date, form_type, extension='pdf')\n",
    "                        else:\n",
    "                            print(f'No suitable file link found: {doc_href}')\n",
    "            if found_10k:\n",
    "                break  # Exit inner loop if 10-K found\n",
    "        if not found_10k:\n",
    "            print(f'No 10-K file type found for filing date: {filing_date}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing each company:   0%|          | 0/92285 [00:00<?, ?it/s]C:\\Users\\lishe\\AppData\\Local\\Temp\\ipykernel_40448\\3555843124.py:94: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  soup = BeautifulSoup(response.content, 'lxml')\n",
      "Processing each company: 100%|██████████| 92285/92285 [22:18:56<00:00,  1.15it/s]    \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import fitz  # PyMuPDF for PDF text extraction\n",
    "\n",
    "# Configuration\n",
    "INDEX_CSV_PATH = r'C:\\Users\\lishe\\Documents\\GitHub\\Programming-Study\\Other\\putnc736bnq5aeat.csv'  # Replace with your index file path\n",
    "SAVE_DIR = 'sec_filings'      # Directory to save downloaded files\n",
    "LOG_FILE = 'download_log.txt' # Log file for recording details of each download\n",
    "USER_AGENT = 'Your Name (your.email@example.com)'  # Replace with your name and email (ASCII only)\n",
    "\n",
    "# Create save directory if it doesn't exist\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Open log file\n",
    "log_file = open(LOG_FILE, 'w')\n",
    "\n",
    "# Read the index data\n",
    "try:\n",
    "    df = pd.read_csv(INDEX_CSV_PATH, dtype=str)\n",
    "    df.columns = df.columns.str.strip().str.lower()  # Clean and standardize column names\n",
    "    log_file.write(f\"Successfully read index file: {INDEX_CSV_PATH}\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to read index file: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Ensure required columns exist\n",
    "if 'cik' not in df.columns or 'form' not in df.columns or 'fdate' not in df.columns:\n",
    "    print(\"Index file missing required columns: 'cik', 'form', or 'fdate'\")\n",
    "    exit(1)\n",
    "\n",
    "# Download and validate function\n",
    "def download_and_validate_file(file_url, cik, filing_date, form_type, extension='html'):\n",
    "    headers = {'User-Agent': USER_AGENT}\n",
    "    response = requests.get(file_url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        file_name = f'{cik}_{filing_date}_{form_type}.{extension}'\n",
    "        file_path = os.path.join(SAVE_DIR, file_name)\n",
    "        with open(file_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        log_file.write(f'File saved to: {file_path}\\n')\n",
    "\n",
    "        # Check content for \"FORM 10-K\", \"10-K\", or \"ANNUAL REPORT\"\n",
    "        if extension == 'pdf':\n",
    "            try:\n",
    "                with fitz.open(file_path) as pdf:\n",
    "                    for page_num in range(min(5, pdf.page_count)):  # Check the first 5 pages\n",
    "                        text = pdf[page_num].get_text()\n",
    "                        if any(keyword in text.upper() for keyword in ['FORM 10-K', '10-K', 'ANNUAL REPORT']):\n",
    "                            log_file.write(f'File contains FORM 10-K: {file_path}\\n')\n",
    "                            return True\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading PDF file: {file_path}, Error: {e}\")\n",
    "        else:\n",
    "            # Check HTML or TXT content\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                    for _ in range(500):  # Read first 500 lines for keywords\n",
    "                        line = file.readline().upper()\n",
    "                        if 'FORM 10-K' in line or '10-K' in line or 'ANNUAL REPORT' in line:\n",
    "                            log_file.write(f'File contains FORM 10-K: {file_path}\\n')\n",
    "                            return True\n",
    "            except UnicodeDecodeError:\n",
    "                print(f\"Cannot decode file: {file_path}, skipping further check.\")\n",
    "        \n",
    "        # Delete file if it doesn't contain the necessary content\n",
    "        log_file.write(f'File does not contain FORM 10-K or related content, deleting: {file_path}\\n')\n",
    "        os.remove(file_path)\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"Failed to download file: {file_url}, Status Code: {response.status_code}\")\n",
    "        return False\n",
    "\n",
    "# Process each CIK and Form in the CSV\n",
    "for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing each company\"):\n",
    "    cik = row['cik'].strip()\n",
    "    form_type = row['form'].strip().upper()\n",
    "    fdate = row['fdate'].strip()  # Expected filing date\n",
    "\n",
    "    if form_type != '10-K':\n",
    "        continue  # Only process 10-K files\n",
    "\n",
    "    # Build company filing page URL\n",
    "    company_url = f'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK={cik}&type={form_type}&count=100&owner=exclude&output=xml'\n",
    "    headers = {'User-Agent': USER_AGENT}\n",
    "    response = requests.get(company_url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(f'Cannot access company page: {company_url}, Status Code: {response.status_code}')\n",
    "        continue\n",
    "\n",
    "    # Parse XML response\n",
    "    soup = BeautifulSoup(response.content, 'lxml')\n",
    "    filings = soup.find_all('filing')\n",
    "\n",
    "    # Iterate through each filing\n",
    "    for filing in filings:\n",
    "        filing_date = filing.find('datefiled').text\n",
    "        if filing_date != fdate:\n",
    "            continue  # Skip files that don't match the fdate\n",
    "\n",
    "        filing_href = filing.find('filinghref').text\n",
    "        log_file.write(f'Found {form_type} file dated {filing_date}: {filing_href}\\n')\n",
    "\n",
    "        # Access filing detail page\n",
    "        response = requests.get(filing_href, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            print(f'Cannot access filing detail page, Status Code: {response.status_code}')\n",
    "            continue\n",
    "\n",
    "        # Parse filing detail page and search for file table\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        doc_tables = soup.find_all('table', class_='tableFile')\n",
    "\n",
    "        # Process each document table for 10-K files\n",
    "        found_10k = False\n",
    "        for doc_table in doc_tables:\n",
    "            rows = doc_table.find_all('tr')\n",
    "            log_file.write(f'Parsing document table with {len(rows)} rows\\n')\n",
    "            for row in rows:\n",
    "                columns = row.find_all('td')\n",
    "                if len(columns) >= 4:\n",
    "                    doc_type = columns[3].text.strip().upper()\n",
    "                    doc_link = columns[2].find('a', href=True)\n",
    "                    \n",
    "                    if doc_type == '10-K' and doc_link:\n",
    "                        found_10k = True\n",
    "                        doc_href = doc_link['href']\n",
    "                        \n",
    "                        # Remove \"/ix?doc=\" if present\n",
    "                        if '/ix?doc=' in doc_href:\n",
    "                            doc_href = doc_href.replace('/ix?doc=', '')\n",
    "\n",
    "                        file_url = f'https://www.sec.gov{doc_href}'\n",
    "                        log_file.write(f'Downloading 10-K file: {file_url}\\n')\n",
    "\n",
    "                        # Determine file extension and download\n",
    "                        if doc_href.endswith('.htm') or doc_href.endswith('.html'):\n",
    "                            download_and_validate_file(file_url, cik, filing_date, form_type, extension='html')\n",
    "                        elif doc_href.endswith('.txt'):\n",
    "                            download_and_validate_file(file_url, cik, filing_date, form_type, extension='txt')\n",
    "                        elif doc_href.endswith('.pdf'):\n",
    "                            download_and_validate_file(file_url, cik, filing_date, form_type, extension='pdf')\n",
    "                        else:\n",
    "                            log_file.write(f'No suitable file link found: {doc_href}\\n')\n",
    "            if found_10k:\n",
    "                break  # Exit inner loop if 10-K found\n",
    "        if not found_10k:\n",
    "            log_file.write(f'No 10-K file type found for filing date: {filing_date}\\n')\n",
    "\n",
    "# Close the log file after all processing\n",
    "log_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
