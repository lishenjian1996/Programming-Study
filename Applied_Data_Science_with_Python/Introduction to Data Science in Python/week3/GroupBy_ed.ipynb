{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping Data\n",
    "\n",
    "Sometimes we want to select data based on groups and understand aggregated data on a group level. We have seen that even though Pandas allows us to iterate over every row in a dataframe, it is generally very slow to do so. Fortunately Pandas has a groupby() function to speed up such task. The idea behind the groupby() function is  that it takes some dataframe, splits it into chunks based on some key values, applies computation on those  chunks, then combines the results back together into another dataframe. In pandas this is referred to as the split-apply-combine pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at an example. First, we'll bring in our pandas and numpy libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at some US census data\n",
    "df = pd.read_csv('datasets/census.csv')\n",
    "# And exclude state level summarizations, which have sum level value of 40\n",
    "df = df[df['SUMLEV']==50]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the first example for groupby() I want to use the census date. Let's get a list of the unique states,\n",
    "# then we can iterate over all the states and for each state we reduce the data frame and calculate the\n",
    "# average.\n",
    "\n",
    "# Let's run such task for 3 times and time it. For this we'll use the cell magic function %%timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 3\n",
    "\n",
    "for state in df['STNAME'].unique():\n",
    "    # We'll just calculate the average using numpy for this particular state\n",
    "    avg = np.average(df.where(df['STNAME']==state).dropna()['CENSUS2010POP'])\n",
    "    # And we'll print it to the screen\n",
    "    print('Counties in state ' + state + \n",
    "          ' have an average population of ' + str(avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you scroll down to the bottom of that output you can see it takes a fair bit of time to finish.\n",
    "# Now let's try another approach using groupby()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 3\n",
    "# For this method, we start by telling pandas we're interested in grouping by state name, this is the \"split\"\n",
    "for group, frame in df.groupby('STNAME'):\n",
    "    # You'll notice there are two values we set here. groupby() returns a tuple, where the first value is the\n",
    "    # value of the key we were trying to group by, in this case a specific state name, and the second one is\n",
    "    # projected dataframe that was found for that group\n",
    "    \n",
    "    # Now we include our logic in the \"apply\" step, which is to calculate an average of the census2010pop\n",
    "    avg = np.average(frame['CENSUS2010POP'])\n",
    "    # And print the results\n",
    "    print('Counties in state ' + group + \n",
    "          ' have an average population of ' + str(avg))\n",
    "# And we don't have to worry about the combine step in this case, because all of our data transformation is\n",
    "# actually printing out results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wow, what a huge difference in speed. An improve by roughly by two factors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, 99% of the time, you'll use group by on one or more columns. But you can also provide a function to\n",
    "# group by and use that to segment your data.\n",
    "\n",
    "# This is a bit of a fabricated example but lets say that you have a big batch job with lots of processing and\n",
    "# you want to work on only a third or so of the states at a given time. We could create some function which\n",
    "# returns a number between zero and two based on the first character of the state name. Then we can tell group\n",
    "# by to use this function to split up our data frame. It's important to note that in order to do this you need\n",
    "# to set the index of the data frame to be the column that you want to group by first.\n",
    "\n",
    "# We'll create some new function called set_batch_number and if the first letter of the parameter is a capital\n",
    "# M we'll return a 0. If it's a capital Q we'll return a 1 and otherwise we'll return a 2. Then we'll pass\n",
    "# this function to the data frame\n",
    "\n",
    "df = df.set_index('STNAME')\n",
    "\n",
    "def set_batch_number(item):\n",
    "    if item[0]<'M':\n",
    "        return 0\n",
    "    if item[0]<'Q':\n",
    "        return 1\n",
    "    return 2\n",
    "\n",
    "# The dataframe is supposed to be grouped by according to the batch number And we will loop through each batch\n",
    "# group\n",
    "for group, frame in df.groupby(set_batch_number):\n",
    "    print('There are ' + str(len(frame)) + ' records in group ' + str(group) + ' for processing.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that this time I didn't pass in a column name to groupby(). Instead, I set the index of the dataframe\n",
    "# to be STNAME, and if no column identifier is passed groupby() will automatically use the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take one more look at an example of how we might group data. In this example, I want to use a dataset\n",
    "# of housing from airbnb. In this dataset there are two columns of interest, one is the cancellation_policy\n",
    "# and the other is the review_scores_value.\n",
    "df=pd.read_csv(\"datasets/listings.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, how would I group by both of these columns? A first approach might be to promote them to a multiindex\n",
    "# and just call groupby()\n",
    "df=df.set_index([\"cancellation_policy\",\"review_scores_value\"])\n",
    "\n",
    "# When we have a multiindex we need to pass in the levels we are interested in grouping by\n",
    "for group, frame in df.groupby(level=(0,1)):\n",
    "    print(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This seems to work ok. But what if we wanted to group by the cancelation policy and review scores, but\n",
    "# separate out all the 10's from those under ten? In this case, we could use a function to manage the\n",
    "# groupings\n",
    "\n",
    "def grouping_fun(item):\n",
    "    # Check the \"review_scores_value\" portion of the index. item is in the format of\n",
    "    # (cancellation_policy,review_scores_value\n",
    "    if item[1] == 10.0:\n",
    "        return (item[0],\"10.0\")\n",
    "    else:\n",
    "        return (item[0],\"not 10.0\")\n",
    "\n",
    "for group, frame in df.groupby(by=grouping_fun):\n",
    "    print(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "To this point we have applied very simple processing to our data after splitting, really just outputting some print statements to demonstrate how the splitting works. The pandas developers have three broad categories of data processing to happen during the apply step, Aggregation of group data, Transformation of group data, and Filtration of group data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The most straight forward apply step is the aggregation of data, and uses the method agg() on the groupby\n",
    "# object. Thus far we have only iterated through the groupby object, unpacking it into a label (the group\n",
    "# name) and a dataframe. But with agg we can pass in a dictionary of the columns we are interested in\n",
    "# aggregating along with the function we are looking to apply to aggregate.\n",
    "\n",
    "# Let's reset the index for our airbnb data\n",
    "df=df.reset_index()\n",
    "\n",
    "# Now lets group by the cancellation policy and find the average review_scores_value by group\n",
    "df.groupby(\"cancellation_policy\").agg({\"review_scores_value\":np.average})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hrm. That didn't seem to work at all. Just a bunch of not a numbers. The issue is actually in the function\n",
    "# that we sent to aggregate. np.average does not ignore nans! However, there is a function we can use for this\n",
    "df.groupby(\"cancellation_policy\").agg({\"review_scores_value\":np.nanmean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can just extend this dictionary to aggregate by multiple functions or multiple columns.\n",
    "df.groupby(\"cancellation_policy\").agg({\"review_scores_value\":(np.nanmean,np.nanstd),\n",
    "                                      \"reviews_per_month\":np.nanmean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a moment to make sure you understand the previous cell, since it's somewhat complex. First we're doing\n",
    "# a group by on the dataframe object by the column \"cancellation_policy\". This creates a new GroupBy object.\n",
    "# Then we are invoking the agg() function on that object. The agg function is going to apply one or more\n",
    "# functions we specify to the group dataframes and return a single row per dataframe/group. When we called\n",
    "# this function we sent it two dictionary entries, each with the key indicating which column we wanted\n",
    "# functions applied to. For the first column we actually supplied a tuple of two functions. Note that these\n",
    "# are not function invocations, like np.nanmean(), or function names, like \"nanmean\" they are references to\n",
    "# functions which will return single values. The groupby object will recognize the tuple and call each\n",
    "# function in order on the same column. The results will be in a heirarchical index, but since they are\n",
    "# columns they don't show as an index per se. Then we indicated another column and a single function we wanted\n",
    "# to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation is different from aggregation. Where agg() returns a single value per column, so one row per\n",
    "# group, tranform() returns an object that is the same size as the group. Essentially, it broadcasts the\n",
    "# function you supply over the grouped dataframe, returning a new dataframe. This makes combining data later\n",
    "# easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For instance, suppose we want to include the average rating values in a given group by cancellation policy,\n",
    "# but preserve the dataframe shape so that we could generate a difference between an individual observation\n",
    "# and the sum.\n",
    "\n",
    "# First, lets define just some subset of columns we are interested in\n",
    "cols=['cancellation_policy','review_scores_value']\n",
    "# Now lets transform it, I'll store this in its own dataframe\n",
    "transform_df=df[cols].groupby('cancellation_policy').transform(np.nanmean)\n",
    "transform_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we can see that the index here is actually the same as the original dataframe. So lets just join this\n",
    "# in. Before we do that, lets rename the column in the transformed version\n",
    "transform_df.rename({'review_scores_value':'mean_review_scores'}, axis='columns', inplace=True)\n",
    "df=df.merge(transform_df, left_index=True, right_index=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Great, we can see that our new column is in place, the mean_review_scores. So now we could create, for\n",
    "# instance, the difference between a given row and it's group (the cancellation policy) means.\n",
    "df['mean_diff']=np.absolute(df['review_scores_value']-df['mean_review_scores'])\n",
    "df['mean_diff'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The GroupBy object has build in support for filtering groups as well. It's often that you'll want to group\n",
    "# by some feature, then make some transformation to the groups, then drop certain groups as part of your\n",
    "# cleaning routines. The filter() function takes in a function which it applies to each group dataframe and\n",
    "# returns either a True or a False, depending upon whether that group should be included in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For instance, if we only want those groups which have a mean rating above 9 included in our results\n",
    "df.groupby('cancellation_policy').filter(lambda x: np.nanmean(x['review_scores_value'])>9.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that the results are still indexed, but that any of the results which were in a group with a mean\n",
    "# review score of less than or equal to 9.2 were not copied over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By far the most common operation I invoke on groupby objects is the apply() function. This allows you to\n",
    "# apply an arbitrary function to each group, and stitch the results back for each apply() into a single\n",
    "# dataframe where the index is preserved.\n",
    "\n",
    "# Lets look at an example using our airbnb data, I'm going to get a clean copy of the dataframe\n",
    "df=pd.read_csv(\"datasets/listings.csv\")\n",
    "# And lets just include some of the columns we were interested in previously\n",
    "df=df[['cancellation_policy','review_scores_value']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In previous work we wanted to find the average review score of a listing and its deviation from the group\n",
    "# mean. This was a two step process, first we used transform() on the groupby object and then we had to\n",
    "# broadcast to create a new column. With apply() we could wrap this logic in one place\n",
    "def calc_mean_review_scores(group):\n",
    "    # group is a dataframe just of whatever we have grouped by, e.g. cancellation policy, so we can treat\n",
    "    # this as the complete dataframe\n",
    "    avg=np.nanmean(group[\"review_scores_value\"])\n",
    "    # now broadcast our formula and create a new column\n",
    "    group[\"review_scores_mean\"]=np.abs(avg-group[\"review_scores_value\"])\n",
    "    return group\n",
    "\n",
    "# Now just apply this to the groups\n",
    "df.groupby('cancellation_policy').apply(calc_mean_review_scores).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using apply can be slower than using some of the specialized functions, especially agg(). But, if your\n",
    "# dataframes are not huge, it's a solid general purpose approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Groupby is a powerful and commonly used tool for data cleaning and data analysis. Once you have grouped the data by some category you have a dataframe of just those values and you can conduct aggregated analysis on the segments that you are interested. The groupby() function follows a split-apply-combine approach - first the data is split into subgroups, then you can apply some transformation, filtering, or aggregation, then the results are combined automatically by pandas for us."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
